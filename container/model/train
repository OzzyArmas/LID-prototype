#!/usr/bin/env python

from models import gmm
import pandas as pd
import os
import traceback
import sys
import pickle
import json
import time


prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

channel = 'training'
training_path = os.path.join(input_path, channel)

def train(model, data_x, data_y):
    '''
    train(x,y)
    '''
    #separate data into snippet_length samples
    #features_es.reshape((int(np.shape(features_es)[0] / sample_length), sample_length, -1))
    for lang in data_y:
        x = data_x[data_y[lang]]
        model.train(x,data_y[lang])
    return model

def get_data():
    '''
    '''
    data_y = {}
    data_x = []
    for idx, language in enumerate(os.listdir(training_path)):
        data_y[language[:-4]] = idx
        data_x.append(pd.read_csv(os.path.join(training_path, language)).as_matrix())
    if len(data_x) == 0:
        raise Exception('no data was loaded')
    return (data_x, data_y)
        
    
def save_model(model, params):
    '''
    dump model to a pickle
    dump pickle on S3
    '''
    file_name = 'gmm' + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime()) + '.plk'
    with open(os.path.join(model_path, file_name), 'w') as out:
        pickle.dump(model, out)
    
def create_model(languages, training_params):
    '''
    read from parampath and figure out how to create model
    hyper parameters for gmm.GMM()
    n_clusters = 30, cov_type='full', iter = 100, snippet_length=75, languages=2
    '''
    clusters = training_params.get('n_clusters', 30)
    covar = training_params.get('cov_type', 'full')
    it = training_params.get('iter', 100)
    snip = training_params.get('snippet_length', 75)
    languages = training_params.get('languages', languages)

    return gmm.GMM(clusters, covar, it, snip, languages)

if __name__ == '__main__':
    try:
        with open(param_path, 'r') as hyper:
            training_params = json.load(hyper)
        
        print(training_params.get('n_clusters', 30))
        raise ('forced')
        # Array of data_x, and data_y values
        data_x, data_y = get_data() 
        print('[SUCCESS] data parsed')
        #create a model based on hyper_params
        model = create_model(len(data_y), training_params)

        print('[SUCCESS] model created')

        #trains model from the parsed s3 data
        model = train(model,data_x, data_y)
        print('[SUCCESS] model trained')
        #Saves model as a pickle
        save_model(model)
        print('[SUCCESS] model saved')

    except Exception as e:
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as err:
            err.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)