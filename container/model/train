#!/usr/bin/env python

from models import gmm
import pandas as pd
import numpy as np
import os
import traceback
import sys
import pickle
import json
import time
import ast
import logging

prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

channel = 'training'
training_path = os.path.join(input_path, channel)

logger = logging.getLogger('instance')
lvl = logging.WARNING

def train(model, data_x, data_y):
    '''
    train(x,y)
    '''
    #separate data into snippet_length samples
    #features_es.reshape((int(np.shape(features_es)[0] / sample_length), sample_length, -1))
    for lang in data_y:
        x = data_x[data_y[lang]]
        shape = np.shape(x)
        logger.log(lvl, f'shape before: {shape}')
        x = np.array(x).reshape(int(shape[0] / model.snippet_length), shape[1])
        logger.log(lvl, f'shape after: {np.shape(x)}')

        model.train(x,data_y[lang])


    return model

def get_data():
    '''
    '''
    data_y = {}
    data_x = []
    for idx, language in enumerate(os.listdir(training_path)):
        data_y[language[:-4]] = idx
        data_x.append(pd.read_csv(os.path.join(training_path, language)).as_matrix())

    if len(data_x) == 0:
        raise Exception('no data was loaded')
    return (data_x, data_y)
        
    
def save_model(model, params):
    '''
    dump model to a pickle
    dump pickle on S3
    '''
    file_name = 'gmm' + time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime()) + '.plk'
    with open(os.path.join(model_path, file_name), 'w') as out:
        pickle.dump(model, out)
    
def create_model(languages, training_params):
    '''
    read from parampath and figure out how to create model
    hyper parameters for gmm.GMM()
    n_clusters = 30, cov_type='full', iter = 100, snippet_length=75, languages=2
    '''
    clusters = ast.literal_eval(training_params.get('n_clusters', 30))
    covar = training_params.get('cov_type', 'full')
    it = training_params.get('iter', 100)
    snip = training_params.get('snippet_length', 75)
    languages = training_params.get('languages', languages)
    logger.log(lvl, type(clusters))
    return gmm.GMM(clusters, covar, it, snip, languages)

if __name__ == '__main__':
    try:
        with open(param_path, 'r') as hyper:
            training_params = json.load(hyper)
        
        # Array of data_x, and data_y values
        data_x, data_y = get_data() 
        logger.log(lvl, '[SUCCESS] data parsed')

        #create a model based on hyper_params
        model = create_model(len(data_y), training_params)
        logger.log(lvl, '[SUCCESS] model created')

        #trains model from the parsed s3 data
        model = train(model,data_x, data_y)
        logger.log(lvl, '[SUCCESS] model trained')
        #Saves model as a pickle
        save_model(model)
        logger.log(lvl, '[SUCCESS] model saved')

    except Exception as e:
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as err:
            err.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)