{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models import lstm\n",
    "import importlib\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "\n",
    "languages = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/oosv/data/csv'\n",
    "data_en = pd.read_csv(f\"{data_path}/english_data.csv\").values.astype(np.float32)\n",
    "data_es = pd.read_csv(f\"{data_path}/spanish_data.csv\").values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4939049, 39)\n",
      "(1760399, 39)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data_en))\n",
    "print(np.shape(data_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.lstm' from '/Users/oosv/prototype/models/lstm.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm.LSTM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "#figure out a way to mix samples up and training data properly\n",
    "def train(samples, labels, model, loss_function, optimizer,  epoch=2):\n",
    "    for e in range(epoch):\n",
    "        for sample, label in zip(samples, labels):\n",
    "            model.zero_grad()\n",
    "            model.hidden = model.init_hidden()\n",
    "            scores = model(sample)\n",
    "            loss = loss_function(scores[-1].view(1,languages), label.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en_sampled = np.reshape(data_en[74:], (-1,75,39))\n",
    "data_es_sampled = np.reshape(data_es[74:], (-1,75,39))\n",
    "en_idx = int(len(data_en_sampled)/4)\n",
    "es_idx = int(len(data_es_sampled)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66994, 75, 39)\n"
     ]
    }
   ],
   "source": [
    "samples_train = np.concatenate((data_en_sampled[en_idx:], \\\n",
    "                                data_es_sampled[es_idx:]), axis = 0)\n",
    "samples_test_en = data_en_sampled[:en_idx]\n",
    "samples_test_es = data_es_sampled[:es_idx]\n",
    "\n",
    "labels_train = []\n",
    "\n",
    "for l in range(len(data_en_sampled[en_idx:])):\n",
    "    labels_train.append(0)\n",
    "\n",
    "for l in range(len(data_es_sampled[es_idx:])):\n",
    "    labels_train.append(1)\n",
    "    \n",
    "labels_test_es = []\n",
    "labels_test_en = []\n",
    "\n",
    "for l in range(len(data_en_sampled[:en_idx])):\n",
    "    labels_test_en.append(0)\n",
    "\n",
    "for l in range(len(data_es_sampled[:es_idx])):\n",
    "    labels_test_es.append(1)\n",
    "    \n",
    "print(np.shape(samples_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison_scary(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shuffle_in_unison_scary(samples_train, labels_train)\n",
    "\n",
    "samples_train = torch.from_numpy(np.array(samples_train).astype(np.float32))\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_test_en = torch.from_numpy(np.array(samples_test_en).astype(np.float32))\n",
    "labels_test_en = torch.tensor(labels_test_en, dtype=torch.float32).long()\n",
    "samples_test_es = torch.from_numpy(np.array(samples_test_es).astype(np.float32))\n",
    "labels_test_es = torch.tensor(labels_test_es, dtype=torch.float32).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 2])\n",
      "tensor([-1.1598, -0.3762], grad_fn=<SelectBackward>)\n",
      "tensor([[-1.1598, -0.3762]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "scores = model(samples_train[0])\n",
    "print(scores.size())\n",
    "print(scores[-1])\n",
    "print(scores[-1].view(1,2))\n",
    "#print(labels[0].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = [np.argmax(model(sample)[-1].view(-1).numpy()) for sample in samples_test]\n",
    "    acc = []\n",
    "    for pre,label in zip(pred,labels_test):\n",
    "        acc.append((pre == label.numpy()))\n",
    "    print(np.average(acc))\n",
    "    #print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_test_en = data_en_sampled[:en_idx]\n",
    "samples_test_es = data_es_sampled[:es_idx]\n",
    "labels_test_es = []\n",
    "labels_test_en = []\n",
    "\n",
    "for l in range(len(samples_test_en)):\n",
    "    labels_test_en.append(0)\n",
    "\n",
    "for l in range(len(samples_test_es)):\n",
    "    labels_test_es.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16463\n",
      "5867\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_test_en))\n",
    "print(len(labels_test_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7266597825426715\n",
      "0.2926538264871314\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    for sample in samples_test_en:\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred.append(np.argmax(model(sample)[-1].view(-1).numpy()))\n",
    "    \n",
    "    acc = []\n",
    "    for pre,label in zip(pred,labels_test_en):\n",
    "        acc.append((pre == label.numpy()))\n",
    "    print(np.average(acc))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    for sample in samples_test_es:\n",
    "        model.hidden = model.init_hidden()\n",
    "        pred.append(np.argmax(model(sample)[-1].view(-1).numpy()))    \n",
    "    \n",
    "    acc = []\n",
    "    for pre,label in zip(pred,labels_test_es):\n",
    "        acc.append((pre == label.numpy()))\n",
    "    print(np.average(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.204676   -12.636937   -11.062954   ...  -0.01432903   0.8103626\n",
      "    0.10494173]\n",
      " [ 10.2578125  -12.017317   -10.990821   ...  -0.0783997    0.57345796\n",
      "    0.5007653 ]\n",
      " [ 10.246252   -11.780709   -11.856711   ...   0.18643181   0.18121667\n",
      "    0.7894497 ]\n",
      " ...\n",
      " [ 10.201228   -11.100131    -6.233317   ...   0.19121367  -0.70668447\n",
      "   -0.44318554]\n",
      " [ 10.150369   -12.443208    -7.726891   ...  -0.05653857  -1.0876395\n",
      "   -0.28177303]\n",
      " [ 10.2936125  -12.076303    -9.61423    ...  -0.12003915  -0.86074924\n",
      "   -0.12338059]]\n",
      "(75, 39)\n"
     ]
    }
   ],
   "source": [
    "trial = data_en_sampled[0]\n",
    "print(trial)\n",
    "print(np.shape(trial))\n",
    "shape = np.shape(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _forward(vec):\n",
    "    global hide\n",
    "    vec = linear1(vec)\n",
    "    vec = sig(vec)\n",
    "    vec = linear2(vec)\n",
    "    vec = linear3(vec)\n",
    "    vec, hide = bilstm(vec.view([vec.size(0),1,-1]), hide)\n",
    "    vec = f.log_softmax(vec)\n",
    "    return vec\n",
    "\n",
    "hidden_dim = 128\n",
    "hide = (torch.zeros(2, 1,hidden_dim // 2),\n",
    "                torch.zeros(2, 1, hidden_dim // 2))\n",
    "linear1 = nn.Linear(shape[1], hidden_dim)\n",
    "sig1 = nn.Sigmoid()\n",
    "linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "sig2 = nn.Sigmoid()\n",
    "linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "sig3 = nn.Sigmoid()\n",
    "bilstm = nn.LSTM(hidden_dim, hidden_dim // 2, bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.2047, -12.6369, -11.0630,  ...,  -0.0143,   0.8104,   0.1049],\n",
      "        [ 10.2578, -12.0173, -10.9908,  ...,  -0.0784,   0.5735,   0.5008],\n",
      "        [ 10.2463, -11.7807, -11.8567,  ...,   0.1864,   0.1812,   0.7894],\n",
      "        ...,\n",
      "        [ 10.2012, -11.1001,  -6.2333,  ...,   0.1912,  -0.7067,  -0.4432],\n",
      "        [ 10.1504, -12.4432,  -7.7269,  ...,  -0.0565,  -1.0876,  -0.2818],\n",
      "        [ 10.2936, -12.0763,  -9.6142,  ...,  -0.1200,  -0.8607,  -0.1234]])\n",
      "torch.Size([75, 39])\n",
      "tensor([[[-4.2746, -4.3007, -4.3653,  ..., -4.3472, -4.2973, -4.3267]],\n",
      "\n",
      "        [[-4.2543, -4.2994, -4.3460,  ..., -4.3404, -4.2912, -4.3465]],\n",
      "\n",
      "        [[-4.2472, -4.3221, -4.3295,  ..., -4.3357, -4.2890, -4.3437]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.2974, -4.3331, -4.3542,  ..., -4.3020, -4.3277, -4.3142]],\n",
      "\n",
      "        [[-4.2992, -4.3244, -4.3568,  ..., -4.2952, -4.3450, -4.2846]],\n",
      "\n",
      "        [[-4.3270, -4.3164, -4.3662,  ..., -4.2900, -4.4134, -4.2802]]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oosv/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "t = (torch.tensor(trial))\n",
    "print(t)\n",
    "print(t.size())\n",
    "print(_forward(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.9098, -26.2261,  -7.2046,  ...,  10.0380,  32.8245, -11.2466],\n",
      "        [  0.9265,  -9.7480,   8.5806,  ...,   4.2106,  23.7612,  16.3050],\n",
      "        [  6.3439,   3.9637,  10.1638,  ..., -35.1688,   0.5335,   4.3044],\n",
      "        ...,\n",
      "        [  6.0424,  18.1639,  -1.0022,  ...,  28.7140,   2.8837,   4.4009],\n",
      "        [-31.0679,  23.2379,  19.6940,  ...,   4.2389, -36.0481, -41.4112],\n",
      "        [ 14.5091,   1.9417, -36.9672,  ...,  -9.8842,  31.5563, -14.5995]])\n"
     ]
    }
   ],
   "source": [
    "ten = torch.randn((10, 75, 39))*16\n",
    "print(ten[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.9098, -26.2261,  -7.2046,  ...,  10.0380,  32.8245, -11.2466],\n",
      "        [  0.9265,  -9.7480,   8.5806,  ...,   4.2106,  23.7612,  16.3050],\n",
      "        [  6.3439,   3.9637,  10.1638,  ..., -35.1688,   0.5335,   4.3044],\n",
      "        ...,\n",
      "        [  6.0424,  18.1639,  -1.0022,  ...,  28.7140,   2.8837,   4.4009],\n",
      "        [-31.0679,  23.2379,  19.6940,  ...,   4.2389, -36.0481, -41.4112],\n",
      "        [ 14.5091,   1.9417, -36.9672,  ...,  -9.8842,  31.5563, -14.5995]])\n",
      "tensor([[ 16.9438,  -6.1944,  -5.3272,  ...,  -0.9229,   7.5183,  23.8152],\n",
      "        [  3.1782,  -4.2258, -10.5094,  ...,  -4.9830,   0.5797,   0.8872],\n",
      "        [-13.8354, -11.5207,  -7.4895,  ...,   6.7079,   2.8402,   0.5392],\n",
      "        ...,\n",
      "        [  7.2859,   3.0824,  12.2979,  ...,   6.9271,  -2.2018, -17.5132],\n",
      "        [  9.7972,  -7.8304,  -3.7164,  ...,   2.6103,   1.0332,   1.9408],\n",
      "        [-13.6335, -22.8914, -18.1936,  ..., -10.4943,  12.6758,  -6.2633]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[ 1.0000, -1.0000, -1.0000,  ..., -0.7273,  1.0000,  1.0000],\n",
      "        [ 0.9965, -0.9996, -1.0000,  ..., -0.9999,  0.5224,  0.7100],\n",
      "        [-1.0000, -1.0000, -1.0000,  ...,  1.0000,  0.9932,  0.4924],\n",
      "        ...,\n",
      "        [ 1.0000,  0.9958,  1.0000,  ...,  1.0000, -0.9758, -1.0000],\n",
      "        [ 1.0000, -1.0000, -0.9988,  ...,  0.9893,  0.7752,  0.9596],\n",
      "        [-1.0000, -1.0000, -1.0000,  ..., -1.0000,  1.0000, -1.0000]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[1.0000e+00, 2.0367e-03, 4.8344e-03,  ..., 2.8437e-01, 9.9946e-01,\n",
      "         1.0000e+00],\n",
      "        [9.6001e-01, 1.4404e-02, 2.7277e-05,  ..., 6.8070e-03, 6.4099e-01,\n",
      "         7.0832e-01],\n",
      "        [9.8027e-07, 9.9222e-06, 5.5862e-04,  ..., 9.9878e-01, 9.4481e-01,\n",
      "         6.3163e-01],\n",
      "        ...,\n",
      "        [9.9932e-01, 9.5616e-01, 1.0000e+00,  ..., 9.9902e-01, 9.9591e-02,\n",
      "         2.4781e-08],\n",
      "        [9.9994e-01, 3.9731e-04, 2.3744e-02,  ..., 9.3152e-01, 7.3754e-01,\n",
      "         8.7445e-01],\n",
      "        [1.1996e-06, 1.1439e-10, 1.2549e-08,  ..., 2.7694e-05, 1.0000e+00,\n",
      "         1.9014e-03]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear1 = nn.Linear(39, 30)\n",
    "out = linear1(ten)\n",
    "tan = nn.Tanh()\n",
    "tan_out = tan(out)\n",
    "sig_out = sig1(out)\n",
    "print(ten[0])\n",
    "print(out[0])\n",
    "print(tan_out[0])\n",
    "print(sig_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0,0,0,0,0] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n",
      "torch.Size([75, 128])\n"
     ]
    }
   ],
   "source": [
    "from models import lstm\n",
    "import importlib\n",
    "importlib.reload(lstm)\n",
    "\n",
    "model = lstm.LSTM()\n",
    "model.train(samples_train[0:10], labels_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 128])\n",
      "tensor([-3.0136, -0.0504])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(samples_train[0])[-1])\n",
    "    print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oosv/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "vec = f.log_softmax(torch.tensor([1,1], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6931, -0.6931])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
