{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import gmm, lstm\n",
    "from extractor import extractor\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oosv/miniconda3/lib/python3.6/site-packages/mkl_fft/_numpy_fft.py:331: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  output = mkl_fft.rfft_numpy(a, n=n, axis=axis)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 (150, 3, 13)\n"
     ]
    }
   ],
   "source": [
    "data_wav = \"/Users/oosv/data/voxforge\"\n",
    "languages = [\"english\", \"spanish\"]\n",
    "TOTAL_FRAMES = 150\n",
    "ENERGY_MIN = 12\n",
    "DELTA_DIM = 3\n",
    "CEPSTRAL_COEF = 13\n",
    "\n",
    "extractor.total_frames = TOTAL_FRAMES\n",
    "extractor.energy_threshold = ENERGY_MIN\n",
    "files = []\n",
    "data_y = []\n",
    "num_samples = 100\n",
    "#get the path for num_samples .wav files\n",
    "for i,l in enumerate(languages):\n",
    "    path = os.path.join(data_wav, l)\n",
    "    for f in os.listdir(path)[:num_samples]:\n",
    "        files.append(os.path.join(path, f))\n",
    "    data_y += [i] * len(os.listdir(path)[:num_samples])\n",
    "\n",
    "print(len(data_y))\n",
    "data_x , rejected = np.array(extractor.make_feature_set(files))\n",
    "print(len(data_x), np.shape(data_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 39\n",
    "hidden = 512\n",
    "languages = 2\n",
    "frames = 75\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x, shape_y = np.shape(data_x), np.shape(data_y)\n",
    "data_x = np.reshape(data_x, [shape_x[0], shape_x[1], shape_x[2] * shape_x[3]])\n",
    "\n",
    "#data_y = np.reshape(data_y, [shape_y[0], shape_y[1], shape_y[2] * shape_y[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models on equal amounts of data\n",
    "shuffle_in_unison(data_x, data_y)\n",
    "train_x = data_x[int(shape_x[0]/4):]\n",
    "test_x = data_x[:int(shape_x[0]/4)]\n",
    "train_y = data_y[int(shape_x[0]/4):]\n",
    "test_y = data_y[:int(shape_x[0]/4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150, 39) (150,)\n",
      "(50, 150, 39) (50,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_x), np.shape(train_y))\n",
    "print(np.shape(test_x), np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_x_sm.npy', 'wb') as file:\n",
    "    np.save(file, train_x)\n",
    "with open('train_y_sm.npy', 'wb') as file:\n",
    "    np.save(file, train_y)\n",
    "with open('test_x_sm.npy', 'wb') as file:\n",
    "    np.save(file, test_x)\n",
    "with open('test_y_sm.npy', 'wb') as file:\n",
    "    np.save(file, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(lstm)\n",
    "_lstm = lstm.LSTM()\n",
    "# _train_data = torch.tensor(train_data)\n",
    "# _train_labels = torch.tensor(train_labels)\n",
    "# print(_train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_data_loader(data, batch_size=100, is_distributed=False, **kwargs):\n",
    "    \n",
    "    train_data_x = torch.tensor(data[0], dtype=torch.float32)\n",
    "    train_data_y = torch.tensor(data[1], dtype=torch.int64)\n",
    "\n",
    "    if is_distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    train_data_x = torch.utils.data.DataLoader(train_data_x, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=False,\n",
    "                                       sampler=train_sampler,\n",
    "                                       **kwargs)\n",
    "\n",
    "    train_data_y = torch.utils.data.DataLoader(train_data_y, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=False,\n",
    "                                       sampler=train_sampler,\n",
    "                                       **kwargs)\n",
    "\n",
    "    return train_data_x, train_data_y\n",
    "\n",
    "def _get_test_data_loader(data, batch_size=100, is_distributed=False, **kwargs):\n",
    "    \n",
    "    test_data_x = torch.tensor(data[0], dtype=torch.float32)\n",
    "    test_data_y = torch.tensor(data[1], dtype=torch.int64)\n",
    "    \n",
    "    if is_distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    test_data_x = torch.utils.data.DataLoader(test_data_x, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=False,\n",
    "                                       sampler=train_sampler,\n",
    "                                       **kwargs)\n",
    "\n",
    "    test_data_y = torch.utils.data.DataLoader(test_data_y, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=False,\n",
    "                                       sampler=train_sampler,\n",
    "                                       **kwargs)\n",
    "         \n",
    "    return test_data_x, test_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_params, train_x, train_y, test_x, test_y):\n",
    "    # set the seed for generating random numbers\n",
    "    device = 'cpu'\n",
    "    train_x, train_y = _get_train_data_loader((train_x, train_y))\n",
    "    test_x, test_y = _get_test_data_loader((test_x, test_y))    \n",
    "    model = create_model(model_params, device)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                        lr=model_params['lr'])\n",
    "    log_interval = 100\n",
    "    is_distributed = None\n",
    "    for epoch in range(2):\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (feature_seq, language) in enumerate(zip(train_x, train_y),1):\n",
    "            # indicate to use this data with GPU\n",
    "            print(len(feature_seq))\n",
    "            print(feature_seq[0].size())\n",
    "            feature_seq = feature_seq.to(device)\n",
    "            language = language.to(device)\n",
    "            \n",
    "            # zero_grad prevents training a new batch\n",
    "            # on the last batch's gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # this calls the forward function, through PyTorch\n",
    "            # output in shape batch_size x 1 x n_languages\n",
    "            scores = model(feature_seq)\n",
    "\n",
    "            # calculate backward loss, get perform gradient descent\n",
    "            loss = loss_function(scores, language.view(-1))\n",
    "            loss.backward()\n",
    "            if is_distributed and not use_cuda:\n",
    "                # average gradients manually for multi-machine cpu case only\n",
    "                _average_gradients(model)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update logging information\n",
    "            if batch_idx % log_interval == 0:\n",
    "                logger.info('Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(model_params, device='cpu'):\n",
    "    '''\n",
    "    :param languages: languages to train lstm over\n",
    "    :param model_params: parameters to use for training LSTM\n",
    "        defaults:\n",
    "            n_features = 39,\n",
    "            n_hidden = 512,\n",
    "            languages = 2, \n",
    "            snippet_length = 75,\n",
    "            dropout=0.0,\n",
    "            bi_directional=False,\n",
    "            num_layers = 1,\n",
    "            linear_layers = 1)\n",
    "    :return: a LSTM model object containing a lstm per language\n",
    "    '''\n",
    "    \n",
    "    # apparently hyperparms loves to give strings, thus int conversions\n",
    "    n_features      =   int(model_params.get('n_features', 39))\n",
    "    n_hidden        =   int(model_params.get('n_hidden', 512))\n",
    "    languages       =   int(model_params.get('languages', 2)) \n",
    "    frames          =   int(model_params.get('frames', 75))\n",
    "    dropout         =   int(model_params.get('dropout', 0.0))\n",
    "    num_lstm_layers =   int(model_params.get('num_layers', 1))\n",
    "    bidirectional   =   bool(model_params.get('bidirectional', False))\n",
    "    num_linear      =   int(model_params.get('num_linear', 1))\n",
    "\n",
    "    # if there is a gpu, the LSTM will take care of checking for that during training\n",
    "    return lstm.LSTM(\n",
    "        n_features,\n",
    "        n_hidden,\n",
    "        languages,\n",
    "        frames,\n",
    "        dropout,\n",
    "        bidirectional,\n",
    "        num_lstm_layers\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={\n",
    "    'n_features' : 39,\n",
    "    'n_hidden'   : 50, \n",
    "    'languages'  : 2,\n",
    "    'frames'     : TOTAL_FRAMES,\n",
    "    'dropout'    : 0,\n",
    "    'num_layers' : 1, \n",
    "    'bidirectional' : False,\n",
    "    'momentum'   : 0.01,\n",
    "    'lr'         : 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "50\n",
      "100\n",
      "torch.Size([150, 39])\n",
      "50\n",
      "torch.Size([150, 39])\n",
      "100\n",
      "torch.Size([150, 39])\n",
      "50\n",
      "torch.Size([150, 39])\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(lstm)\n",
    "train(model_params, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
