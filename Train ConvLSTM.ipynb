{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training a ConvLSTM Deep Network\n",
    "\n",
    "###### Model Specifically trains a Convolutional LSTM model\n",
    "see model_params for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import time\n",
    "\n",
    "role         = get_execution_role()\n",
    "sess         = sage.Session()\n",
    "bucket       = 'oosv-multilingual-bucket'\n",
    "TOTAL_FRAMES = 150\n",
    "\n",
    "\n",
    "# this is where to find training and testing date and their respective channels\n",
    "# when the instance launches, it will create a folder \n",
    "# /opt/ml/input/data/{channel}/ where all files in the buckets below are will be copied over\n",
    "# TODO: move debug data to a debug bucket, currently this is downloading all data in folders which is bad\n",
    "train_args = {\n",
    "    'training' : f's3://{bucket}/data/train/train_5',\n",
    "    'validation'  : f's3://{bucket}/data/test/test_5'\n",
    "}\n",
    "debug_args = {\n",
    "    'training' : f's3://{bucket}/data/debug/train/',\n",
    "    'validation'  : f's3://{bucket}/data/debug/test'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_mixed_model_params={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 1024, \n",
    "    'languages'       : 5,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0.0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 25,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'MixedLSTM',\n",
    "}\n",
    "\n",
    "# No Pooling 3 Channels\n",
    "best_conv_model_params={ \n",
    "    'n_features'      : 13,\n",
    "    'n_hidden'        : 1024, \n",
    "    'languages'       : 5,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 25,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'ConvLSTM',\n",
    "    'kernel'          : (1,7),\n",
    "    'output_channels' : 1\n",
    "}\n",
    "\n",
    "estimator1 = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 500,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/MixedLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"MixedLSTM\",\n",
    "                    hyperparameters=best_mixed_model_params)\n",
    "\n",
    "estimator2 = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 500,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/ConvLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"ConvLSTM-Last\",\n",
    "                    hyperparameters=best_conv_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: MixedLSTM-2018-12-13-18-47-32-536\n"
     ]
    }
   ],
   "source": [
    "estimator1.fit(train_args, wait = False)\n",
    "#estimator2.fit(train_args, wait = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sage-maker-debug-convlstm-2018-12-13-00-31-05-889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-13 00:31:06 Starting - Starting the training job."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4ba2bedc273d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                     hyperparameters=model_params_debug)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mestimator_debug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   1106\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Debugging single instance\n",
    "model_params_debug={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 5,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'ConvLSTM'\n",
    "}\n",
    "estimator_debug = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.8xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/output',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"sage-maker-debug-convlstm\",\n",
    "                    hyperparameters=model_params_debug)\n",
    "\n",
    "estimator_debug.fit(debug_args, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging single instance\n",
    "model_params1={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0.0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'ConvLSTM',\n",
    "    'kernel'          : (1,7),\n",
    "    'output_channels' : 1\n",
    "}\n",
    "\n",
    "# Debugging single instance\n",
    "model_params2={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'ConvLSTM',\n",
    "    'kernel'          : (1,7),\n",
    "    'output_channels' : 2\n",
    "}\n",
    "\n",
    "# Debugging single instance\n",
    "model_params3={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'ConvLSTM',\n",
    "    'kernel'          : (1,7), \n",
    "    'output_channels' : 5\n",
    "}\n",
    "\n",
    "model_params4={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000,\n",
    "    'model'           : 'ConvLSTM',\n",
    "    'kernel'          : (3,3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator1 = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/ConvLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"ConvLSTM\",\n",
    "                    hyperparameters=model_params1)\n",
    "\n",
    "estimator2 = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/ConvLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"ConvLSTM\",\n",
    "                    hyperparameters=model_params2)\n",
    "\n",
    "estimator3 = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/ConvLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"ConvLSTM\",\n",
    "                    hyperparameters=model_params3)\n",
    "\n",
    "estimator4 = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/ConvLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"ConvLSTM\",\n",
    "                    hyperparameters=model_params4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ConvLSTM-2018-10-22-23-09-43-774\n",
      "INFO:sagemaker:Creating training-job with name: ConvLSTM-2018-10-22-23-09-44-786\n",
      "INFO:sagemaker:Creating training-job with name: ConvLSTM-2018-10-22-23-09-46-103\n"
     ]
    }
   ],
   "source": [
    "estimator1.fit(train_args, wait = False)\n",
    "estimator2.fit(train_args, wait = False)\n",
    "estimator3.fit(train_args, wait = False)\n",
    "#estimator4.fit(train_args, wait = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section below if for testing best performance using Mixed LSTM and Conv LSTM Models over 6 Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
