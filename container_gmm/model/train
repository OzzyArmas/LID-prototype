#!/usr/bin/env python

from models import gmm
import pandas as pd
import numpy as np
import os
import traceback
import sys
import pickle
import json
import time
import ast
import logging

# important SageMaker predefined paths
prefix = '/opt/ml/'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

'''
setting training path, there's other channels for
evaluation and testing. Generally these only useful when 
the training uses PIPE mode instead of FILE mode; gmm 
model is currently set up to use FILE as there isn't
incredibily large amounts of data yet. PIPE should most definitely 
be used when training data exceeds 10 GB
'''
channel = 'training'
training_path = os.path.join(input_path, channel)
train_data = os.path.join(training_path, 'train')
test_data = os.path.join(training_path, 'test')

logger = logging.getLogger('instance')
lvl = logging.WARNING
lvl_e = logging.ERROR

# this variable is used to check if training job
# will run until completion only using about 10 samples
debug = True
debug_length = 2000

# TODO: Think of a way to write this file only once
# rather than a different version for each container

def eval(model):
    '''
    :param model: trained gmm model to be trained
    :return: accuracy from the model
    '''
    data_y = {}
    data_x = []

    for idx, language in enumerate(os.listdir(test_data)):
        data_y[language[:-4]] = idx
        
        if debug:
            data_x.append(
                pd.read_csv(
                    os.path.join(test_data, language)
                    ).as_matrix()[:debug_length])
        else:
            data_x.append(
                pd.read_csv(
                    os.path.join(test_data, language)
                    ).as_matrix())

    if len(data_x) == 0:
        raise Exception('no data was loaded')
    
    #accuracy
    acc = {}
    for lang in data_y:
        pred = model.predict_all(data_x[data_y[lang]])
        acc[lang] = np.average(pred == data_y[lang])
    return acc

def train(model, data_x, data_y):
    '''
    :param data_x: x values to train GMM over, given the nature of the GMM,
        it can be fed all utterances as we are assuming they are all 
        indepedent of one another for this Naive Bayes model
    :param data_y: labels corresponding to the which GMM 
        should be used to train this model
    :return: a trained model containing all GMM
    '''
    #separate data into snippet_length samples
    #features_es.reshape((int(np.shape(features_es)[0] / sample_length), sample_length, -1))
    for lang in data_y:
        x = data_x[data_y[lang]]
        model.train(x,data_y[lang])


    return model

def get_data():
    '''
    Saves training data locally to train model
    if debug is on, it only uses debug_length rows of data
    '''
    logger.log(lvl, f'debug2: {debug}')
    data_y = {}
    data_x = []
    for idx, language in enumerate(os.listdir(train_data)):
        # files are currently saved as <language>.csv, we save the language
        # for legibility
        data_y[language[:-4]] = idx
        if debug:
            data_x.append(
                pd.read_csv(
                    os.path.join(train_data,language)
                    ).as_matrix()[:debug_length])
        else:
            data_x.append(
                pd.read_csv(
                    os.path.join(train_data, language)
                    ).as_matrix())

    if len(data_x) == 0:
        raise Exception('no data was loaded')

    return (data_x, data_y)
        
    
def save_model(model, acc):
    '''
    :param model: GMM object model to save as a .plk file
    :param acc: dict of accuracy per language
    model and acc are saved in the model_path provided by SageMaker
    which is then saved in an S3 bucket as model.tar.gz
    '''
    file_name = f'''gmm_clust-{str(model.clusters)}.plk'''

    with open(os.path.join(model_path, file_name), 'wb') as out:
        pickle.dump(model, out)
    
    for lang in acc:
        acc[lang] = float(acc[lang])
    
    file_name = f'''accuracy.json'''

    with open(os.path.join(model_path, file_name), 'w') as out:
        json.dump(acc, out)

def create_model(languages, training_params):
    '''
    :param languages: languages to train gmm over
    :param training_params: parameters to use for training GMM
        n_clusters = 1024, cov_type='full', iter = 100,
        snippet_length=75, languages=2
    :return: a GMM model object containing a gmm per language
    '''
    clusters = training_params.get('n_clusters', 1024)
    
    # the json file may convert the array given in the hyperparams json into a 
    # a string, so this is just to circumvent that problem
    if isinstance(clusters, str):
        clusters = ast.literal_eval(clusters)
    
    # apparently hyperparms loves to give strings
    covar = training_params.get('cov_type', 'full')
    it = int(training_params.get('iter', 100))
    snip = int(training_params.get('snippet_length', 75))
    languages = int(training_params.get('languages', languages))
    
    return gmm.GMM(clusters, covar, it, snip, languages)

if __name__ == '__main__':
    try:
        with open(param_path, 'r') as hyper:
            training_params = json.load(hyper)
        
        logger.log(lvl, f'debug : {debug}' )

        # Array of data_x, and data_y values
        data_x, data_y = get_data() 
        logger.log(lvl, '[SUCCESS] data parsed')
        #create a model based on hyper_params
        model = create_model(len(data_y), training_params)
        logger.log(lvl, '[SUCCESS] model created')

        #trains model from the parsed s3 data
        model = train(model,data_x, data_y)
        logger.log(lvl, '[SUCCESS] model trained')

        #saves a checkpoint, in case model fails in testing 
        save_model(model, {})
        logger.log(lvl, '[SUCCESS] checkpoint saved')

        #Tries test data to measure accuracy and evaluate model
        acc = eval(model)
        logger.log(lvl, f'[SUCCESS] acc: {acc}')

        #save model as a pickle
        save_model(model, acc)
        logger.log(lvl, '[SUCCESS] model saved')

    except Exception as e:
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as err:
            err.write('Exception during training: ' + str(e) + '\n' + trc)
            logger.log(lvl_e, 'Exception during training: ' + str(e) + '\n' + trc)
        print('Exception during training: ' + str(e) + '\n' + trc)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)