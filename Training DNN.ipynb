{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training a Costume Mixed LSTM Deep Network\n",
    "\n",
    "###### Model Specifically trains a costume LSTM model\n",
    "see model_params for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import time\n",
    "\n",
    "role         = get_execution_role()\n",
    "sess         = sage.Session()\n",
    "bucket       = 'oosv-multilingual-bucket'\n",
    "TOTAL_FRAMES = 150\n",
    "\n",
    "\n",
    "# this is where to find training and testing date and their respective channels\n",
    "# when the instance launches, it will create a folder \n",
    "# /opt/ml/input/data/{channel}/ where all files in the buckets below are will be copied over\n",
    "# TODO: move debug data to a debug bucket, currently this is downloading all data in folders which is bad\n",
    "train_args = {\n",
    "    'training' : f's3://{bucket}/data/train',\n",
    "    'validation'  : f's3://{bucket}/data/test'\n",
    "}\n",
    "debug_args = {\n",
    "    'training' : f's3://{bucket}/data/debug/train',\n",
    "    'validation'  : f's3://{bucket}/data/debug/test'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22 21:31:41 Starting - Starting the training job...\n",
      "2018-10-22 21:31:45 Starting - Launching requested ML instances.........\n",
      "2018-10-22 21:33:22 Starting - Preparing the instances for training......\n",
      "2018-10-22 21:34:41 Downloading - Downloading input data\n",
      "2018-10-22 21:34:41 Training - Downloading the training image...\n",
      "2018-10-22 21:35:07 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:08,527 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:08,604 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:08,812 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:09,096 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:09,096 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:09,096 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:09,097 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -vvv -U . \u001b[0m\n",
      "\u001b[31mCreated temporary directory: /tmp/pip-ephem-wheel-cache-p1lvyyj2\u001b[0m\n",
      "\u001b[31mCreated temporary directory: /tmp/pip-req-tracker-m8nvu_y2\u001b[0m\n",
      "\u001b[31mCreated requirements tracker '/tmp/pip-req-tracker-m8nvu_y2'\u001b[0m\n",
      "\u001b[31mCreated temporary directory: /tmp/pip-install-xwi44ncq\u001b[0m\n",
      "\u001b[31mProcessing /tmp/tmptxfdgrvq/module_dir\n",
      "  Created temporary directory: /tmp/pip-req-build-zrnmz2z5\n",
      "  Added file:///tmp/tmptxfdgrvq/module_dir to build tracker '/tmp/pip-req-tracker-m8nvu_y2'\n",
      "  Running setup.py (path:/tmp/pip-req-build-zrnmz2z5/setup.py) egg_info for package from file:///tmp/tmptxfdgrvq/module_dir\n",
      "    Running command python setup.py egg_info\n",
      "    WARNING: '' not a valid package name; please use only .-separated package names in setup.py\n",
      "    running egg_info\n",
      "    creating pip-egg-info/train.egg-info\n",
      "    writing top-level names to pip-egg-info/train.egg-info/top_level.txt\n",
      "    writing dependency_links to pip-egg-info/train.egg-info/dependency_links.txt\n",
      "    writing pip-egg-info/train.egg-info/PKG-INFO\n",
      "    writing manifest file 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '__pycache__*' found under directory '.'\n",
      "    warning: no previously-included files matching '*.pyc' found under directory '.'\n",
      "    warning: no previously-included files matching '*.pyo' found under directory '.'\n",
      "    writing manifest file 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-req-build-zrnmz2z5 has version 1.0.0, which satisfies requirement train==1.0.0 from file:///tmp/tmptxfdgrvq/module_dir\n",
      "  Removed train==1.0.0 from file:///tmp/tmptxfdgrvq/module_dir from build tracker '/tmp/pip-req-tracker-m8nvu_y2'\u001b[0m\n",
      "\u001b[31mCould not parse version from link: file:///tmp/tmptxfdgrvq/module_dir\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Created temporary directory: /tmp/pip-wheel-6hy0nl8g\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Destination directory: /tmp/pip-wheel-6hy0nl8g\n",
      "  Running command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-req-build-zrnmz2z5/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-6hy0nl8g --python-tag cp35\n",
      "  WARNING: '' not a valid package name; please use only .-separated package names in setup.py\n",
      "  running bdist_wheel\n",
      "  The [wheel] section is deprecated. Use [bdist_wheel] instead.\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  copying train_mixedlstm.py -> build/lib\n",
      "  copying train_convlstm.py -> build/lib\n",
      "  copying setup.py -> build/lib\n",
      "  copying train.py -> build/lib\n",
      "  running egg_info\n",
      "  creating train.egg-info\n",
      "  writing train.egg-info/PKG-INFO\n",
      "  writing top-level names to train.egg-info/top_level.txt\n",
      "  writing dependency_links to train.egg-info/dependency_links.txt\n",
      "  writing manifest file 'train.egg-info/SOURCES.txt'\n",
      "  reading manifest file 'train.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '__pycache__*' found under directory '.'\n",
      "  warning: no previously-included files matching '*.pyc' found under directory '.'\n",
      "  warning: no previously-included files matching '*.pyo' found under directory '.'\n",
      "  writing manifest file 'train.egg-info/SOURCES.txt'\n",
      "  copying MANIFEST.in -> build/lib\n",
      "  copying setup.cfg -> build/lib\n",
      "  copying pip-delete-this-directory.txt -> build/lib\n",
      "  creating build/lib/build\n",
      "  creating build/lib/build/lib\n",
      "  copying build/lib/setup.py -> build/lib/build/lib\n",
      "  copying build/lib/train.py -> build/lib/build/lib\n",
      "  copying build/lib/train_convlstm.py -> build/lib/build/lib\n",
      "  copying build/lib/train_mixedlstm.py -> build/lib/build/lib\n",
      "  creating build/lib/models\n",
      "  copying models/__init__.py -> build/lib/models\n",
      "  copying models/convlstm.py -> build/lib/models\n",
      "  copying models/gmm.py -> build/lib/models\n",
      "  copying models/lstm.py -> build/lib/models\n",
      "  creating build/lib/pip-egg-info\n",
      "  creating build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/PKG-INFO -> build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/SOURCES.txt -> build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/dependency_links.txt -> build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/top_level.txt -> build/lib/pip-egg-info/train.egg-info\n",
      "  creating build/lib/train.egg-info\n",
      "  copying train.egg-info/PKG-INFO -> build/lib/train.egg-info\n",
      "  copying train.egg-info/SOURCES.txt -> build/lib/train.egg-info\n",
      "  copying train.egg-info/dependency_links.txt -> build/lib/train.egg-info\n",
      "  copying train.egg-info/top_level.txt -> build/lib/train.egg-info\n",
      "  warning: build_py: byte-compiling is disabled, skipping.\n",
      "\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/train_mixedlstm.py -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/pip-delete-this-directory.txt -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/__init__.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/lstm.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/gmm.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/convlstm.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/train_convlstm.py -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/setup.py -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/MANIFEST.in -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/setup.cfg -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/pip-egg-info\n",
      "  creating build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/top_level.txt -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/PKG-INFO -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  creating build/bdist.linux-x86_64/wheel/build\n",
      "  creating build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/train_mixedlstm.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/train_convlstm.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/setup.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/train.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  creating build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/top_level.txt -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/PKG-INFO -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.py -> build/bdist.linux-x86_64/wheel\n",
      "  warning: install_lib: byte-compiling is disabled, skipping.\n",
      "\n",
      "  running install_egg_info\n",
      "  Copying train.egg-info to build/bdist.linux-x86_64/wheel/train-1.0.0-py3.5.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/train-1.0.0.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-6hy0nl8g/train-1.0.0-py2.py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'MANIFEST.in'\n",
      "  adding 'pip-delete-this-directory.txt'\n",
      "  adding 'setup.cfg'\n",
      "  adding 'setup.py'\n",
      "  adding 'train.py'\n",
      "  adding 'train_convlstm.py'\n",
      "  adding 'train_mixedlstm.py'\n",
      "  adding 'build/lib/setup.py'\n",
      "  adding 'build/lib/train.py'\n",
      "  adding 'build/lib/train_convlstm.py'\n",
      "  adding 'build/lib/train_mixedlstm.py'\n",
      "  adding 'models/__init__.py'\n",
      "  adding 'models/convlstm.py'\n",
      "  adding 'models/gmm.py'\n",
      "  adding 'models/lstm.py'\n",
      "  adding 'pip-egg-info/train.egg-info/PKG-INFO'\n",
      "  adding 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "  adding 'pip-egg-info/train.egg-info/dependency_links.txt'\n",
      "  adding 'pip-egg-info/train.egg-info/top_level.txt'\n",
      "  adding 'train.egg-info/PKG-INFO'\n",
      "  adding 'train.egg-info/SOURCES.txt'\n",
      "  adding 'train.egg-info/dependency_links.txt'\n",
      "  adding 'train.egg-info/top_level.txt'\n",
      "  adding 'train-1.0.0.dist-info/METADATA'\n",
      "  adding 'train-1.0.0.dist-info/WHEEL'\n",
      "  adding 'train-1.0.0.dist-info/top_level.txt'\n",
      "  adding 'train-1.0.0.dist-info/RECORD'\n",
      "  removing build/bdist.linux-x86_64/wheel\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p1lvyyj2/wheels/73/e4/fd/335abedf5c93a8434fa9e8a134fb42e46fca25fe0e84e376b5\n",
      "  Removing source in /tmp/pip-req-build-zrnmz2z5\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\n",
      "\n",
      "  Removing source in /tmp/pip-install-xwi44ncq/train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mCleaning up...\u001b[0m\n",
      "\u001b[31mRemoved build tracker '/tmp/pip-req-tracker-m8nvu_y2'\u001b[0m\n",
      "\u001b[31m2018-10-22 21:35:10,393 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"job_name\": \"sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"module_name\": \"train\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"log_level\": 20,\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-367698673629/sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743/source/sourcedir.tar.gz\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"num_gpus\": 8,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"num_cpus\": 64,\n",
      "    \"hyperparameters\": {\n",
      "        \"linear_layers\": 1,\n",
      "        \"batch-size\": 100,\n",
      "        \"n_features\": 39,\n",
      "        \"lr\": 0.001,\n",
      "        \"languages\": 2,\n",
      "        \"dropout\": 0,\n",
      "        \"epoch\": 5,\n",
      "        \"backend\": \"gloo\",\n",
      "        \"frames\": 150,\n",
      "        \"test-batch-size\": 1000,\n",
      "        \"n_hidden\": 512,\n",
      "        \"lstm_layers\": 1,\n",
      "        \"bidirectional\": true\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ]\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HP_TEST-BATCH-SIZE=1000\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_HP_FRAMES=150\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"100\",\"--bidirectional\",\"True\",\"--dropout\",\"0\",\"--epoch\",\"5\",\"--frames\",\"150\",\"--languages\",\"2\",\"--linear_layers\",\"1\",\"--lr\",\"0.001\",\"--lstm_layers\",\"1\",\"--n_features\",\"39\",\"--n_hidden\",\"512\",\"--test-batch-size\",\"1000\"]\u001b[0m\n",
      "\u001b[31mSM_HP_LANGUAGES=2\u001b[0m\n",
      "\u001b[31mSM_HP_LSTM_LAYERS=1\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HPS={\"backend\":\"gloo\",\"batch-size\":100,\"bidirectional\":true,\"dropout\":0,\"epoch\":5,\"frames\":150,\"languages\":2,\"linear_layers\":1,\"lr\":0.001,\"lstm_layers\":1,\"n_features\":39,\"n_hidden\":512,\"test-batch-size\":1000}\u001b[0m\n",
      "\u001b[31mSM_HP_BATCH-SIZE=100\u001b[0m\n",
      "\u001b[31mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":100,\"bidirectional\":true,\"dropout\":0,\"epoch\":5,\"frames\":150,\"languages\":2,\"linear_layers\":1,\"lr\":0.001,\"lstm_layers\":1,\"n_features\":39,\"n_hidden\":512,\"test-batch-size\":1000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-367698673629/sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"ethwe\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[31mSM_HP_DROPOUT=0\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_HP_BIDIRECTIONAL=true\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\",\"validation\"]\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_HP_N_FEATURES=39\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HP_N_HIDDEN=512\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-367698673629/sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_HP_LINEAR_LAYERS=1\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --backend gloo --batch-size 100 --bidirectional True --dropout 0 --epoch 5 --frames 150 --languages 2 --linear_layers 1 --lr 0.001 --lstm_layers 1 --n_features 39 --n_hidden 512 --test-batch-size 1000\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-10-22 21:36:09 Uploading - Uploading generated training model\n",
      "2018-10-22 21:36:09 Failed - Training job failed\n",
      "\u001b[31mNCCL version 2.1.15+cuda9.0\u001b[0m\n",
      "\u001b[31mException during training: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/train.py\", line 460, in <module>\n",
      "    train(parser.parse_args())\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/train.py\", line 250, in train\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\u001b[0m\n",
      "\u001b[31mRuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26\n",
      "\u001b[0m\n",
      "\u001b[31m2018-10-22 21:36:03,573 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[31mCommand \"/usr/bin/python -m train --backend gloo --batch-size 100 --bidirectional True --dropout 0 --epoch 5 --frames 150 --languages 2 --linear_layers 1 --lr 0.001 --lstm_layers 1 --n_features 39 --n_hidden 512 --test-batch-size 1000\"\u001b[0m\n",
      "\u001b[31mGet train data loader\u001b[0m\n",
      "\u001b[31mGet test data loader\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/models/lstm.py:132: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.hidden))\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31m/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\u001b[0m\n",
      "\u001b[31mTHCudaCheck FAIL file=/pytorch/aten/src/THC/generic/THCTensorMath.cu line=26 error=59 : device-side assert triggered\u001b[0m\n",
      "\u001b[31mException during training: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26\u001b[0m\n",
      "\u001b[31mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/train.py\", line 460, in <module>\n",
      "    train(parser.parse_args())\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/train.py\", line 250, in train\n",
      "    loss.backward()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/tensor.py\", line 93, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\", line 89, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\u001b[0m\n",
      "\u001b[31mRuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error training sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743: Failed Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/bin/python -m train --backend gloo --batch-size 100 --bidirectional True --dropout 0 --epoch 5 --frames 150 --languages 2 --linear_layers 1 --lr 0.001 --lstm_layers 1 --n_features 39 --n_hidden 512 --test-batch-size 1000\"\nGet train data loader\nGet test data loader\n/usr/local/lib/python3.5/dist-packages/models/lstm.py:132: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n  self.hidden))\n/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b1324c82f17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                     hyperparameters=model_params_debug)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mestimator_debug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TrainingJobStatus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Completed'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Stopped'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FailureReason'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(No reason provided)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error training {}: {} Reason: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error training sage-maker-debug-mixedlstm-2018-10-22-21-31-40-743: Failed Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/bin/python -m train --backend gloo --batch-size 100 --bidirectional True --dropout 0 --epoch 5 --frames 150 --languages 2 --linear_layers 1 --lr 0.001 --lstm_layers 1 --n_features 39 --n_hidden 512 --test-batch-size 1000\"\nGet train data loader\nGet test data loader\n/usr/local/lib/python3.5/dist-packages/models/lstm.py:132: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n  self.hidden))\n/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, "
     ]
    }
   ],
   "source": [
    "# Debugging single instance\n",
    "model_params_debug={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 2,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : True,\n",
    "    'lr'              : 0.001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 5,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "estimator_debug = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.8xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/output',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"sage-maker-debug-mixedlstm\",\n",
    "                    hyperparameters=model_params_debug)\n",
    "\n",
    "estimator_debug.fit(debug_args, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512 hidden dimensions, lr = 0.001, single lstm single direction, single linear layer\n",
    "# Reason: Understanding simple LSTM performance\n",
    "model_params_1={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,  # Needs 2 LSTM Layers for it to be non-zero\n",
    "    'lstm_layers'     : 1,  \n",
    "    'linear_layers'   : 1,  # Minimum of 1\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "\n",
    "# Google: 2560 hidden dimensions, 4 linear,  lr = 0.0001, single lstm single direction. Same as google Paper using fusion\n",
    "# Reason: Seeing how a paper ML Algorithm performs, although orignal document used 3000 hours of data\n",
    "model_params_2={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 2560, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 4,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 1000,\n",
    "    'epoch'           : 10,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "\n",
    "# 512 hidden dimensions, 1 linear,  lr = 0.001, double lstm single direction, dropout = 0.5\n",
    "# Reason: the point of this model is to see how dropout affects performance\n",
    "model_params_3={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0.5,\n",
    "    'lstm_layers'      : 2, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "\n",
    "# 512 hidden dimensions, 1 linear,  lr = 0.001, single BiLSTM, dropout = 0.0\n",
    "# Reason: the point of this model is to see how BiLSTM affects performance\n",
    "model_params_4={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 3,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : True,\n",
    "    'lr'              : 0.001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an estimtor for each type of parameter dictionary\n",
    "#\n",
    "# Entry_point represents a .py file that will run as script/executable\n",
    "#\n",
    "# role=sagemaker.get_execution_role()\n",
    "#\n",
    "# train_instance_type and train_intance_count are self explantory\n",
    "#\n",
    "# Volume size is how much non-ram memory is to be used\n",
    "#\n",
    "# source_dir = 'deep_learning', representing which directory\n",
    "#    from the working directory of this notebook to copy over\n",
    "#\n",
    "# output_path = which bucket and folder to save the output model and any other meta data to\n",
    "#    output path will save a {training_job_name}/output/model.tar.gz folder, model.tar.gz will have model and metadata\n",
    "#\n",
    "# hyperparameters = represents any way to possibly \n",
    "#    modify the training job externally, these are past in as arguments\n",
    "\n",
    "estimator1 = PyTorch(entry_point = 'train.py',\n",
    "                    role = role,\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir = 'deep_learning',\n",
    "                    output_path = f's3://{bucket}/MixedLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"MixedLSTM\",\n",
    "                    hyperparameters = model_params_1)\n",
    "\n",
    "# Google based model\n",
    "estimator2 = PyTorch(entry_point = 'train.py',\n",
    "                    role = role,\n",
    "                    train_instance_count = 12,\n",
    "                    train_instance_type = 'ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir = 'deep_learning',\n",
    "                    output_path = f's3://{bucket}/MixedLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"MixedLSTM\",\n",
    "                    hyperparameters = model_params_2)\n",
    "\n",
    "estimator3 = PyTorch(entry_point = 'train.py',\n",
    "                    role = role,\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir = 'deep_learning',\n",
    "                    output_path = f's3://{bucket}/MixedLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"MixedLSTM\",\n",
    "                    hyperparameters = model_params_3)\n",
    "\n",
    "estimator4 = PyTorch(entry_point = 'train.py',\n",
    "                    role = role,\n",
    "                    train_instance_count = 1,\n",
    "                    train_instance_type = 'ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir = 'deep_learning',\n",
    "                    output_path = f's3://{bucket}/MixedLSTM',\n",
    "                    framework_version=0.4,\n",
    "                    base_job_name=\"MixedLSTM\",\n",
    "                    hyperparameters = model_params_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: MixedLSTM-2018-10-22-21-51-30-545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-22 21:51:31 Starting - Starting the training job...\n",
      "2018-10-22 21:51:32 Starting - Launching requested ML instances...............\n",
      "2018-10-22 21:54:04 Starting - Preparing the instances for training......\n",
      "2018-10-22 21:55:27 Downloading - Downloading input data............\n",
      "2018-10-22 21:57:33 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,248 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,324 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,533 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,884 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,884 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,884 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:34,884 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -vvv -U . \u001b[0m\n",
      "\u001b[31mCreated temporary directory: /tmp/pip-ephem-wheel-cache-2qrq8f1_\u001b[0m\n",
      "\u001b[31mCreated temporary directory: /tmp/pip-req-tracker-ibqskszx\u001b[0m\n",
      "\u001b[31mCreated requirements tracker '/tmp/pip-req-tracker-ibqskszx'\u001b[0m\n",
      "\u001b[31mCreated temporary directory: /tmp/pip-install-o_bkipzg\u001b[0m\n",
      "\u001b[31mProcessing /tmp/tmptxacbee4/module_dir\n",
      "  Created temporary directory: /tmp/pip-req-build-15ye0426\n",
      "  Added file:///tmp/tmptxacbee4/module_dir to build tracker '/tmp/pip-req-tracker-ibqskszx'\n",
      "  Running setup.py (path:/tmp/pip-req-build-15ye0426/setup.py) egg_info for package from file:///tmp/tmptxacbee4/module_dir\n",
      "    Running command python setup.py egg_info\n",
      "    WARNING: '' not a valid package name; please use only .-separated package names in setup.py\n",
      "    running egg_info\n",
      "    creating pip-egg-info/train.egg-info\n",
      "    writing top-level names to pip-egg-info/train.egg-info/top_level.txt\n",
      "    writing pip-egg-info/train.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/train.egg-info/dependency_links.txt\n",
      "    writing manifest file 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '__pycache__*' found under directory '.'\n",
      "    warning: no previously-included files matching '*.pyc' found under directory '.'\n",
      "    warning: no previously-included files matching '*.pyo' found under directory '.'\n",
      "    writing manifest file 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-req-build-15ye0426 has version 1.0.0, which satisfies requirement train==1.0.0 from file:///tmp/tmptxacbee4/module_dir\n",
      "  Removed train==1.0.0 from file:///tmp/tmptxacbee4/module_dir from build tracker '/tmp/pip-req-tracker-ibqskszx'\u001b[0m\n",
      "\u001b[31mCould not parse version from link: file:///tmp/tmptxacbee4/module_dir\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Created temporary directory: /tmp/pip-wheel-xj072zdl\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Destination directory: /tmp/pip-wheel-xj072zdl\n",
      "  Running command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-req-build-15ye0426/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-xj072zdl --python-tag cp35\n",
      "  WARNING: '' not a valid package name; please use only .-separated package names in setup.py\n",
      "  running bdist_wheel\n",
      "  The [wheel] section is deprecated. Use [bdist_wheel] instead.\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  copying train_convlstm.py -> build/lib\n",
      "  copying train_mixedlstm.py -> build/lib\n",
      "  copying setup.py -> build/lib\n",
      "  copying train.py -> build/lib\n",
      "  running egg_info\n",
      "  creating train.egg-info\n",
      "  writing top-level names to train.egg-info/top_level.txt\n",
      "  writing train.egg-info/PKG-INFO\n",
      "  writing dependency_links to train.egg-info/dependency_links.txt\n",
      "  writing manifest file 'train.egg-info/SOURCES.txt'\n",
      "  reading manifest file 'train.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '__pycache__*' found under directory '.'\n",
      "  warning: no previously-included files matching '*.pyc' found under directory '.'\n",
      "  warning: no previously-included files matching '*.pyo' found under directory '.'\n",
      "  writing manifest file 'train.egg-info/SOURCES.txt'\n",
      "  copying MANIFEST.in -> build/lib\n",
      "  copying setup.cfg -> build/lib\n",
      "  copying pip-delete-this-directory.txt -> build/lib\n",
      "  creating build/lib/build\n",
      "  creating build/lib/build/lib\n",
      "  copying build/lib/setup.py -> build/lib/build/lib\n",
      "  copying build/lib/train.py -> build/lib/build/lib\n",
      "  copying build/lib/train_convlstm.py -> build/lib/build/lib\n",
      "  copying build/lib/train_mixedlstm.py -> build/lib/build/lib\n",
      "  creating build/lib/models\n",
      "  copying models/__init__.py -> build/lib/models\n",
      "  copying models/convlstm.py -> build/lib/models\n",
      "  copying models/gmm.py -> build/lib/models\n",
      "  copying models/lstm.py -> build/lib/models\n",
      "  creating build/lib/pip-egg-info\n",
      "  creating build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/PKG-INFO -> build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/SOURCES.txt -> build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/dependency_links.txt -> build/lib/pip-egg-info/train.egg-info\n",
      "  copying pip-egg-info/train.egg-info/top_level.txt -> build/lib/pip-egg-info/train.egg-info\n",
      "  creating build/lib/train.egg-info\n",
      "  copying train.egg-info/PKG-INFO -> build/lib/train.egg-info\n",
      "  copying train.egg-info/SOURCES.txt -> build/lib/train.egg-info\n",
      "  copying train.egg-info/dependency_links.txt -> build/lib/train.egg-info\n",
      "  copying train.egg-info/top_level.txt -> build/lib/train.egg-info\n",
      "  warning: build_py: byte-compiling is disabled, skipping.\n",
      "\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/MANIFEST.in -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/PKG-INFO -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/train.egg-info/top_level.txt -> build/bdist.linux-x86_64/wheel/train.egg-info\n",
      "  copying build/lib/pip-delete-this-directory.txt -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/build\n",
      "  creating build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/train_convlstm.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/train_mixedlstm.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/setup.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/build/lib/train.py -> build/bdist.linux-x86_64/wheel/build/lib\n",
      "  copying build/lib/setup.cfg -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/train_convlstm.py -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/gmm.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/__init__.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/convlstm.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/models/lstm.py -> build/bdist.linux-x86_64/wheel/models\n",
      "  copying build/lib/train_mixedlstm.py -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/pip-egg-info\n",
      "  creating build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/PKG-INFO -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/pip-egg-info/train.egg-info/top_level.txt -> build/bdist.linux-x86_64/wheel/pip-egg-info/train.egg-info\n",
      "  copying build/lib/setup.py -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib/train.py -> build/bdist.linux-x86_64/wheel\n",
      "  warning: install_lib: byte-compiling is disabled, skipping.\n",
      "\n",
      "  running install_egg_info\n",
      "  Copying train.egg-info to build/bdist.linux-x86_64/wheel/train-1.0.0-py3.5.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/train-1.0.0.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-xj072zdl/train-1.0.0-py2.py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'MANIFEST.in'\n",
      "  adding 'pip-delete-this-directory.txt'\n",
      "  adding 'setup.cfg'\n",
      "  adding 'setup.py'\n",
      "  adding 'train.py'\n",
      "  adding 'train_convlstm.py'\n",
      "  adding 'train_mixedlstm.py'\n",
      "  adding 'build/lib/setup.py'\n",
      "  adding 'build/lib/train.py'\n",
      "  adding 'build/lib/train_convlstm.py'\n",
      "  adding 'build/lib/train_mixedlstm.py'\n",
      "  adding 'models/__init__.py'\n",
      "  adding 'models/convlstm.py'\n",
      "  adding 'models/gmm.py'\n",
      "  adding 'models/lstm.py'\n",
      "  adding 'pip-egg-info/train.egg-info/PKG-INFO'\n",
      "  adding 'pip-egg-info/train.egg-info/SOURCES.txt'\n",
      "  adding 'pip-egg-info/train.egg-info/dependency_links.txt'\n",
      "  adding 'pip-egg-info/train.egg-info/top_level.txt'\n",
      "  adding 'train.egg-info/PKG-INFO'\n",
      "  adding 'train.egg-info/SOURCES.txt'\n",
      "  adding 'train.egg-info/dependency_links.txt'\n",
      "  adding 'train.egg-info/top_level.txt'\n",
      "  adding 'train-1.0.0.dist-info/METADATA'\n",
      "  adding 'train-1.0.0.dist-info/WHEEL'\n",
      "  adding 'train-1.0.0.dist-info/top_level.txt'\n",
      "  adding 'train-1.0.0.dist-info/RECORD'\n",
      "  removing build/bdist.linux-x86_64/wheel\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2qrq8f1_/wheels/77/d6/d8/059f3574200d08f520af9afaef1db3988e715c0e57eaa428e9\n",
      "  Removing source in /tmp/pip-req-build-15ye0426\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\n",
      "\n",
      "  Removing source in /tmp/pip-install-o_bkipzg/train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mCleaning up...\u001b[0m\n",
      "\u001b[31mRemoved build tracker '/tmp/pip-req-tracker-ibqskszx'\u001b[0m\n",
      "\u001b[31m2018-10-22 21:57:36,327 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"ethwe\",\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"num_gpus\": 8,\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-367698673629/MixedLSTM-2018-10-22-21-51-30-545/source/sourcedir.tar.gz\",\n",
      "    \"log_level\": 20,\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\"\n",
      "        }\n",
      "    },\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"job_name\": \"MixedLSTM-2018-10-22-21-51-30-545\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"linear_layers\": 1,\n",
      "        \"languages\": 3,\n",
      "        \"n_hidden\": 512,\n",
      "        \"lr\": 0.0001,\n",
      "        \"bidirectional\": false,\n",
      "        \"batch-size\": 100,\n",
      "        \"frames\": 150,\n",
      "        \"dropout\": 0,\n",
      "        \"lstm_layers\": 1,\n",
      "        \"epoch\": 20,\n",
      "        \"n_features\": 39,\n",
      "        \"backend\": \"gloo\",\n",
      "        \"test-batch-size\": 1000\n",
      "    }\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HP_N_FEATURES=39\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_HPS={\"backend\":\"gloo\",\"batch-size\":100,\"bidirectional\":false,\"dropout\":0,\"epoch\":20,\"frames\":150,\"languages\":3,\"linear_layers\":1,\"lr\":0.0001,\"lstm_layers\":1,\"n_features\":39,\"n_hidden\":512,\"test-batch-size\":1000}\u001b[0m\n",
      "\u001b[31mSM_HP_LINEAR_LAYERS=1\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"batch-size\":100,\"bidirectional\":false,\"dropout\":0,\"epoch\":20,\"frames\":150,\"languages\":3,\"linear_layers\":1,\"lr\":0.0001,\"lstm_layers\":1,\"n_features\":39,\"n_hidden\":512,\"test-batch-size\":1000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"MixedLSTM-2018-10-22-21-51-30-545\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-367698673629/MixedLSTM-2018-10-22-21-51-30-545/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"ethwe\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_HP_N_HIDDEN=512\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_HP_FRAMES=150\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[31mSM_HP_LSTM_LAYERS=1\u001b[0m\n",
      "\u001b[31mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[31mSM_HP_TEST-BATCH-SIZE=1000\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-367698673629/MixedLSTM-2018-10-22-21-51-30-545/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\",\"validation\"]\u001b[0m\n",
      "\u001b[31mSM_HP_LR=0.0001\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_HP_DROPOUT=0\u001b[0m\n",
      "\u001b[31mSM_HP_LANGUAGES=3\u001b[0m\n",
      "\u001b[31mSM_HP_BATCH-SIZE=100\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--batch-size\",\"100\",\"--bidirectional\",\"False\",\"--dropout\",\"0\",\"--epoch\",\"20\",\"--frames\",\"150\",\"--languages\",\"3\",\"--linear_layers\",\"1\",\"--lr\",\"0.0001\",\"--lstm_layers\",\"1\",\"--n_features\",\"39\",\"--n_hidden\",\"512\",\"--test-batch-size\",\"1000\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCH=20\u001b[0m\n",
      "\u001b[31mSM_HP_BIDIRECTIONAL=false\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --backend gloo --batch-size 100 --bidirectional False --dropout 0 --epoch 20 --frames 150 --languages 3 --linear_layers 1 --lr 0.0001 --lstm_layers 1 --n_features 39 --n_hidden 512 --test-batch-size 1000\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNCCL version 2.1.15+cuda9.0\u001b[0m\n",
      "\u001b[31m2018-10-22 22:09:16,835 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2018-10-22 22:09:24 Uploading - Uploading generated training model\n",
      "2018-10-22 22:09:24 Completed - Training job completed\n",
      "Billable seconds: 837\n"
     ]
    }
   ],
   "source": [
    "estimator1.fit(train_args, wait = True)\n",
    "\n",
    "# estimator3.fit(train_args, wait = False)\n",
    "\n",
    "# estimator4.fit(train_args, wait = False)\n",
    "\n",
    "# estimator2.fit(train_args, wait = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-10-18-23-50-20-680\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-10-18-23-50-21-641\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2018-10-18-23-50-22-933\n"
     ]
    }
   ],
   "source": [
    "# 512 Hidden dimensions, 1 linear layer, lr = 0.001, double layer BiLSTM, dropout=0.5, 5 epochs, \n",
    "# Reason: After testing the model with BiLSTM's and Dropout, both resulted on useful gains\n",
    "# in accuracy, additionally, BiLSTM showed reduced compute time relative to 2 layers LSTM.\n",
    "# This however, should balance out, depending on performance, lr will be changed\n",
    "# this model is on hold until the best learning rate is found\n",
    "model_params_5={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 2,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "# Pure reduced learning rate to .0001 learning rate\n",
    "model_params_6={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 2,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0.5,\n",
    "    'lstm_layers'     : 2, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : False,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "\n",
    "# BiLSTM + Dropout, increased number of nodes as BiLSTM will reduce them\n",
    "model_params_7={ \n",
    "    'n_features'      : 39,\n",
    "    'n_hidden'        : 512, \n",
    "    'languages'       : 2,\n",
    "    'frames'          : TOTAL_FRAMES,\n",
    "    'dropout'         : 0,\n",
    "    'lstm_layers'     : 1, \n",
    "    'linear_layers'   : 1,\n",
    "    'bidirectional'   : True,\n",
    "    'lr'              : 0.0001,\n",
    "    'batch-size'      : 100,\n",
    "    'epoch'           : 20,\n",
    "    'backend'         : 'gloo',\n",
    "    'test-batch-size' : 1000\n",
    "}\n",
    "estimator5 = PyTorch(entry_point='train_mixedlstm.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/output',\n",
    "                    framework_version=0.4,\n",
    "                    hyperparameters=model_params_5)\n",
    "\n",
    "estimator6 = PyTorch(entry_point='train_mixedlstm.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/output',\n",
    "                    framework_version=0.4,\n",
    "                    hyperparameters=model_params_6)\n",
    "\n",
    "estimator7 = PyTorch(entry_point='train_mixedlstm.py',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p3.16xlarge',\n",
    "                    train_volume_size = 70,\n",
    "                    source_dir='deep_learning',\n",
    "                    output_path= f's3://{bucket}/output',\n",
    "                    framework_version=0.4,\n",
    "                    hyperparameters=model_params_7)\n",
    "estimator5.fit(train_args, wait = False)\n",
    "estimator6.fit(train_args, wait = False)\n",
    "estimator7.fit(train_args, wait = False)\n",
    "# TODO: Add a sixth option called \"use inception, and that may be \n",
    "# what I need, but if Inception then I may have to change\n",
    "# data to be 3 dimensional... maybe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a function to get data from s3 buckets\n",
    "def get_data(file_name, bucket, _dir='train'):\n",
    "    prefix = '/tmp/data/'\n",
    "    path = f'data/{_dir}/'\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket).download_file(path + file_name, prefix + file_name)\n",
    "    arr = np.load(prefix + file_name)\n",
    "    os.remove(prefix + file_name)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to check if the data being fed to train_mixedlstm.py is correctly shaped\n",
    "# Shape should be n_samples x TOTAL_FRAMES (150) x n_features (39)\n",
    "# there are files saved the same as below but with _sm.npy for doing job completion tests\n",
    "# before running them with gigabytes worth of data\n",
    "train_x = get_data('train_x.npy', bucket)\n",
    "train_y = get_data('train_y.npy', bucket)\n",
    "test_x  = get_data('test_x.npy', bucket, 'test')\n",
    "test_y  = get_data('test_y.npy', bucket, 'test')\n",
    "print(np.shape(train_x))\n",
    "print(np.shape(train_y))\n",
    "print(np.shape(test_x))\n",
    "print(np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
